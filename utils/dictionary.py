import os
import json
import urllib.request
import urllib.parse
import time
from typing import List, Dict, Any
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class StablecoinDictionary:
    """
    ìŠ¤í…Œì´ë¸”ì½”ì¸ ìš©ì–´ ë°±ê³¼ì‚¬ì „ RAG ì‹œìŠ¤í…œ
    stablecoin_book_2025_full.md íŒŒì¼ ê¸°ë°˜ìœ¼ë¡œ vector dbë¥¼ êµ¬ì¶•í•˜ê³  ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€
    ë°±ê³¼ì‚¬ì „ì— ì—†ëŠ” ë‚´ìš©ì€ ì¸í„°ë„· ê²€ìƒ‰ìœ¼ë¡œ ë³´ì™„
    """
    
    def __init__(self):
        self.embeddings = OpenAIEmbeddings()
        self.llm = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.1,
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        self.vector_store = None
        self.qa_chain = None
        self._initialize_knowledge_base()
    
    def _load_markdown_content(self) -> List[Document]:
        """stablecoin_book_2025_full.md íŒŒì¼ì„ êµ¬ì¡°í™”ëœ ë¬¸ì„œë¡œ ë¡œë“œ"""
        try:
            file_path = os.path.join(os.path.dirname(__file__), "stablecoin_book_2025_full.md")
            with open(file_path, 'r', encoding='utf-8') as file:
                content = file.read()
            
            # ë§ˆí¬ë‹¤ìš´ì„ êµ¬ì¡°í™”ëœ ì„¹ì…˜ìœ¼ë¡œ ë¶„í• 
            documents = []
            
            # ì„¹ì…˜ë³„ë¡œ ë¶„í•  (## ê¸°ì¤€)
            sections = content.split('## ')
            
            for i, section in enumerate(sections):
                if i == 0:  # ì²« ë²ˆì§¸ ì„¹ì…˜ (ì œëª©)
                    if section.strip():
                        title = section.strip().replace('# ', '').replace('\n', ' ').strip()
                        documents.append(Document(
                            page_content=section.strip(),
                            metadata={
                                "source": "stablecoin_book_2025_full.md",
                                "section": "title",
                                "title": title,
                                "section_index": i
                            }
                        ))
                else:
                    # ì„¹ì…˜ ì œëª©ê³¼ ë‚´ìš© ë¶„ë¦¬
                    lines = section.strip().split('\n')
                    if lines:
                        section_title = lines[0].strip()
                        section_content = '\n'.join(lines[1:]).strip()
                        
                        if section_content:
                            # ìš©ì–´ë³„ë¡œ ì¶”ê°€ ë¶„í•  (ë³¼ë“œ í…ìŠ¤íŠ¸ ê¸°ì¤€)
                            terms = self._extract_terms_from_section(section_content)
                            
                            if terms:
                                # ê° ìš©ì–´ë¥¼ ê°œë³„ ë¬¸ì„œë¡œ ìƒì„±
                                for term_data in terms:
                                    documents.append(Document(
                                        page_content=f"ì„¹ì…˜: {section_title}\nìš©ì–´: {term_data['term']}\nì •ì˜: {term_data['definition']}\nì˜ˆì‹œ: {term_data['examples']}",
                                        metadata={
                                            "source": "stablecoin_book_2025_full.md",
                                            "section": section_title,
                                            "term": term_data['term'],
                                            "section_index": i,
                                            "term_type": term_data.get('type', 'general')
                                        }
                                    ))
                            else:
                                # ìš©ì–´ê°€ ì—†ëŠ” ê²½ìš° ì„¹ì…˜ ì „ì²´ë¥¼ ë¬¸ì„œë¡œ ìƒì„±
                                documents.append(Document(
                                    page_content=f"ì„¹ì…˜: {section_title}\në‚´ìš©: {section_content}",
                                    metadata={
                                        "source": "stablecoin_book_2025_full.md",
                                        "section": section_title,
                                        "section_index": i,
                                        "term_type": "section_content"
                                    }
                                ))
            
            return documents
            
        except Exception as e:
            print(f"ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def _extract_terms_from_section(self, section_content: str) -> List[Dict[str, Any]]:
        """ì„¹ì…˜ ë‚´ìš©ì—ì„œ ìš©ì–´ì™€ ì •ì˜ë¥¼ ì¶”ì¶œ"""
        terms = []
        
        # ì¤„ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        lines = section_content.split('\n')
        current_term = None
        current_definition = []
        current_examples = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # ë³¼ë“œ í…ìŠ¤íŠ¸ë¡œ ëœ ìš©ì–´ ì°¾ê¸°
            if line.startswith('- **') and line.endswith('**'):
                # ì´ì „ ìš©ì–´ ì €ì¥
                if current_term and current_definition:
                    terms.append({
                        'term': current_term,
                        'definition': ' '.join(current_definition),
                        'examples': current_examples,
                        'type': 'definition'
                    })
                
                # ìƒˆ ìš©ì–´ ì‹œì‘
                current_term = line[4:-3].strip()  # - **ìš©ì–´** ì—ì„œ ìš©ì–´ ë¶€ë¶„ ì¶”ì¶œ
                current_definition = []
                current_examples = []
                
            elif line.startswith('- **') and '**' in line:
                # ìš©ì–´ì™€ ì •ì˜ê°€ í•œ ì¤„ì— ìˆëŠ” ê²½ìš°
                parts = line.split('**')
                if len(parts) >= 3:
                    term = parts[1].strip()
                    definition = parts[2].strip()
                    if definition.startswith(':'):
                        definition = definition[1:].strip()
                    
                    terms.append({
                        'term': term,
                        'definition': definition,
                        'examples': [],
                        'type': 'inline_definition'
                    })
                    
            elif current_term and line.startswith('  - '):
                # ì˜ˆì‹œ í•­ëª©
                example = line[4:].strip()
                current_examples.append(example)
                
            elif current_term and line.startswith('- '):
                # ì •ì˜ í•­ëª©
                definition = line[2:].strip()
                current_definition.append(definition)
                
            elif current_term and not line.startswith('  - ') and not line.startswith('- '):
                # ì¼ë°˜ í…ìŠ¤íŠ¸ (ì •ì˜ì˜ ì¼ë¶€)
                current_definition.append(line)
        
        # ë§ˆì§€ë§‰ ìš©ì–´ ì €ì¥
        if current_term and current_definition:
            terms.append({
                'term': current_term,
                'definition': ' '.join(current_definition),
                'examples': current_examples,
                'type': 'definition'
            })
        
        return terms
    
    def _initialize_knowledge_base(self):
        """ìŠ¤í…Œì´ë¸”ì½”ì¸ ìš©ì–´ ë°±ê³¼ì‚¬ì „ ì§€ì‹ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        print("ğŸ”„ ìŠ¤í…Œì´ë¸”ì½”ì¸ ìš©ì–´ ë°±ê³¼ì‚¬ì „ ì§€ì‹ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        
        # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ êµ¬ì¡°í™”ëœ ë¬¸ì„œë¡œ ë¡œë“œ
        documents = self._load_markdown_content()
        
        if not documents:
            print("âš ï¸ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            # í´ë°±: ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©
            sample_docs = [
                Document(
                    page_content="ìš©ì–´: ìŠ¤í…Œì´ë¸”ì½”ì¸\nì •ì˜: ê°€ê²© ë³€ë™ì„±ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ íŠ¹ì • ìì‚°ì— ê°€ì¹˜ë¥¼ ê³ ì •í•œ ì•”í˜¸í™”í\nì˜ˆì‹œ: USDT, USDC, DAI",
                    metadata={"source": "sample", "term": "ìŠ¤í…Œì´ë¸”ì½”ì¸", "section": "ê¸°ë³¸ ìš©ì–´"}
                ),
                Document(
                    page_content="ìš©ì–´: USDT\nì •ì˜: í…Œë”ì‚¬ì—ì„œ ë°œí–‰í•˜ëŠ” 1:1 USD í˜ê¹… ìŠ¤í…Œì´ë¸”ì½”ì¸\nì˜ˆì‹œ: ê±°ë˜ì†Œ ê±°ë˜, ì†¡ê¸ˆ, ê²°ì œ",
                    metadata={"source": "sample", "term": "USDT", "section": "ê¸°ë³¸ ìš©ì–´"}
                )
            ]
            documents = sample_docs
        
        print(f"ğŸ“š ì´ {len(documents)}ê°œì˜ ë¬¸ì„œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        
        # í…ìŠ¤íŠ¸ ë¶„í•  (ë” ì‘ì€ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ì •í™•ë„ í–¥ìƒ)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,  # ë” ì‘ì€ ì²­í¬ë¡œ ë³€ê²½
            chunk_overlap=100,
            separators=["\n\n", "\n", ". ", " ", ""]
        )
        splits = text_splitter.split_documents(documents)
        
        print(f"âœ‚ï¸ í…ìŠ¤íŠ¸ë¥¼ {len(splits)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")
        
        # Vector DB ìƒì„±
        print("ğŸ” FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì¤‘...")
        self.vector_store = FAISS.from_documents(splits, self.embeddings)
        
        # QA ì²´ì¸ ìƒì„± (ìƒˆë¡œìš´ ë°©ì‹ ì‚¬ìš©)
        retriever = self.vector_store.as_retriever(search_kwargs={"k": 8})
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
        prompt_template = """
        ë‹¤ìŒì€ ìŠ¤í…Œì´ë¸”ì½”ì¸ ìš©ì–´ ë°±ê³¼ì‚¬ì „ì— ëŒ€í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤.
        ì§ˆë¬¸: {question}
        
        ì œê³µëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
        ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , í•„ìš”ì‹œ ì˜ˆì‹œë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.
        
        ì»¨í…ìŠ¤íŠ¸: {context}
        """
        
        # ìƒˆë¡œìš´ ì²´ì¸ ìƒì„±
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True,
            chain_type_kwargs={"prompt": PromptTemplate.from_template(prompt_template)}
        )
        
        print("âœ… ìŠ¤í…Œì´ë¸”ì½”ì¸ ìš©ì–´ ë°±ê³¼ì‚¬ì „ ì§€ì‹ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ!")
        
        # í†µê³„ ì •ë³´ ì¶œë ¥
        self._print_statistics()
    
    def _print_statistics(self):
        """ë²¡í„° DB í†µê³„ ì •ë³´ ì¶œë ¥"""
        try:
            if self.vector_store:
                # ì¹´í…Œê³ ë¦¬ë³„ ìš©ì–´ ìˆ˜ ì§‘ê³„
                categories = {}
                sample_queries = ["ìŠ¤í…Œì´ë¸”ì½”ì¸", "ë¸”ë¡ì²´ì¸", "ê·œì œ", "ê¸°ìˆ ", "ì‹œì¥", "DeFi", "CBDC"]
                
                for query in sample_queries:
                    docs = self.vector_store.similarity_search(query, k=10)
                    for doc in docs:
                        section = doc.metadata.get('section', 'ê¸°íƒ€')
                        if section not in categories:
                            categories[section] = 0
                        if doc.metadata.get('term'):
                            categories[section] += 1
                
                print("\nğŸ“Š ë°±ê³¼ì‚¬ì „ í†µê³„:")
                for category, count in sorted(categories.items()):
                    if count > 0:
                        print(f"  â€¢ {category}: {count}ê°œ ìš©ì–´")
                
                print(f"  â€¢ ì´ ë¬¸ì„œ ìˆ˜: {len(self.vector_store.docstore._dict)}")
                
        except Exception as e:
            print(f"í†µê³„ ì •ë³´ ì¶œë ¥ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _search_internet(self, query: str) -> str:
        """ì¸í„°ë„·ì—ì„œ ìŠ¤í…Œì´ë¸”ì½”ì¸ ê´€ë ¨ ì •ë³´ ê²€ìƒ‰"""
        try:
            # Tavily ìš°ì„  ì‚¬ìš© (ìœ ì¼í•œ ì™¸ë¶€ ê²€ìƒ‰ API)
            tavily_api_key = os.getenv("TAVILY_API_KEY")
            if tavily_api_key:
                return self._tavily_search(query)
            
            # Tavily API í‚¤ ë¯¸ì„¤ì • ì‹œ ì•Œë¦¼ ë°˜í™˜
            return "Tavily API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ TAVILY_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."
            
        except Exception as e:
            return f"ì¸í„°ë„· ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def _tavily_search(self, query: str) -> str:
        """Tavily Search APIë¥¼ ì‚¬ìš©í•œ ì›¹ ê²€ìƒ‰"""
        try:
            tavily_api_key = os.getenv("TAVILY_API_KEY")
            if not tavily_api_key:
                return "Tavily API í‚¤ê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ TAVILY_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”."
            
            url = "https://api.tavily.com/search"
            payload = {
                "api_key": tavily_api_key,
                "query": query,
                "search_depth": "advanced",
                "max_results": 5,
                "include_answer": True,
                "include_images": False
            }
            data = json.dumps(payload).encode("utf-8")
            
            req = urllib.request.Request(url, data=data, headers={"Content-Type": "application/json"}, method="POST")
            with urllib.request.urlopen(req, timeout=20) as resp:
                result = json.loads(resp.read().decode("utf-8"))
            
            # ê²°ê³¼ ì •ë¦¬
            parts = []
            if result.get("answer"):
                parts.append(result["answer"])
            sources = result.get("results", [])
            if sources:
                formatted_sources = []
                for item in sources[:5]:
                    title = item.get("title") or item.get("url") or "ì¶œì²˜"
                    url_src = item.get("url", "")
                    snippet = item.get("content", "")
                    if snippet:
                        snippet = snippet[:180] + ("..." if len(snippet) > 180 else "")
                    formatted_sources.append(f"- {title} ({url_src})\n  ìš”ì•½: {snippet}")
                parts.append("\nì°¸ê³  ì¶œì²˜:\n" + "\n".join(formatted_sources))
            
            return "\n\n".join(parts) if parts else "ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ê¸° ì–´ë ¤ì› ìŠµë‹ˆë‹¤."
        except Exception as e:
            return f"Tavily ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {str(e)}"
    
    def _check_knowledge_coverage(self, question: str, answer: str) -> bool:
        """ë‹µë³€ì´ ì§€ì‹ë² ì´ìŠ¤ì—ì„œ ì¶©ë¶„íˆ ë„ì¶œë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        # ê°„ë‹¨í•œ í’ˆì§ˆ ì²´í¬: ë‹µë³€ì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ì¼ë°˜ì ì¸ ê²½ìš°
        if len(answer) < 50:
            return False
        
        # ì‚¬ê³¼/ì •ë³´ë¶€ì¡± í‘œí˜„ì´ í¬í•¨ëœ ê²½ìš°
        low_confidence_phrases = [
            "ëª¨ë¥´ê² ìŠµë‹ˆë‹¤", "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤",
            "ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "í™•ì‹¤í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤",
            "ì œê³µëœ ì •ë³´ì—ëŠ”", "ì œê³µë°›ì€ ì •ë³´ì—ëŠ”", "í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤",
            "í•´ë‹¹ ì •ë³´ë¥¼ ì œê³µí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
        ]
        
        for phrase in low_confidence_phrases:
            if phrase in answer:
                return False
        
        return True
    
    def _is_in_knowledge_base(self, question: str) -> bool:
        """ì§ˆë¬¸ì´ ì§€ì‹ë² ì´ìŠ¤ì— ìˆëŠ” ë‚´ìš©ì¸ì§€ ë¹ ë¥´ê²Œ í™•ì¸"""
        try:
            if not self.vector_store:
                return False
            
            # ë¹ ë¥¸ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ í™•ì¸ (k=2ë¡œ ì¤„ì—¬ì„œ ì†ë„ í–¥ìƒ)
            docs = self.vector_store.similarity_search(question, k=2)
            
            # ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„° í™•ì¸
            for doc in docs:
                metadata = doc.metadata
                # stablecoin_book_2025_full.mdì—ì„œ ì˜¨ ë¬¸ì„œì¸ì§€ í™•ì¸
                if (metadata.get('source') == 'stablecoin_book_2025_full.md' and 
                    metadata.get('term') and 
                    len(doc.page_content) > 100):  # ì¶©ë¶„í•œ ë‚´ìš©ì´ ìˆëŠ” ë¬¸ì„œ
                    return True
            
            return False
            
        except Exception as e:
            print(f"ì§€ì‹ë² ì´ìŠ¤ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def get_fast_answer(self, question: str) -> str:
        """ë¹ ë¥¸ ë‹µë³€ì„ ìœ„í•œ ìµœì í™”ëœ í•¨ìˆ˜ (DBì— ìˆëŠ” ë‚´ìš©ì¸ ê²½ìš°)"""
        start_time = time.time()
        
        try:
            # ì§€ì‹ë² ì´ìŠ¤ì— ìˆëŠ” ë‚´ìš©ì¸ì§€ ë¹ ë¥´ê²Œ í™•ì¸
            is_in_kb = self._is_in_knowledge_base(question)
            
            if is_in_kb:
                # DBì— ìˆëŠ” ë‚´ìš©ì¸ ê²½ìš° - ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ë¡œ ë¹ ë¥¸ ë‹µë³€
                fast_prompt = f"""
                ì§ˆë¬¸: {question}
                
                ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ë°±ê³¼ì‚¬ì „ì˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
                ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , í•µì‹¬ ë‚´ìš© ìœ„ì£¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.
                """
                
                # ë¹ ë¥¸ ê²€ìƒ‰ì„ ìœ„í•´ k=3ìœ¼ë¡œ ì œí•œ
                result = self.qa_chain({"query": fast_prompt})
                answer = result["result"]
                
                response_time = time.time() - start_time
                print(f"âš¡ ë¹ ë¥¸ ë‹µë³€ ì™„ë£Œ (ì‘ë‹µì‹œê°„: {response_time:.2f}ì´ˆ)")
                
                return answer
            else:
                # DBì— ì—†ëŠ” ë‚´ìš©ì¸ ê²½ìš° ì¼ë°˜ í•¨ìˆ˜ í˜¸ì¶œ
                return self.get_answer(question)
                
        except Exception as e:
            response_time = time.time() - start_time
            print(f"âŒ ë¹ ë¥¸ ë‹µë³€ ì˜¤ë¥˜ (ì‘ë‹µì‹œê°„: {response_time:.2f}ì´ˆ)")
            return f"ë¹ ë¥¸ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def get_answer(self, question: str) -> str:
        """ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±"""
        start_time = time.time()
        
        try:
            # ë¨¼ì € ì§€ì‹ë² ì´ìŠ¤ì— ìˆëŠ” ë‚´ìš©ì¸ì§€ ë¹ ë¥´ê²Œ í™•ì¸
            is_in_kb = self._is_in_knowledge_base(question)
            
            if not is_in_kb:
                # KBì— ì—†ìœ¼ë©´ ì¦‰ì‹œ ì›¹ ê²€ìƒ‰ ê²½ë¡œë¡œ ì „í™˜
                internet_result = self._search_internet(question)
                enhanced_prompt = f"""
                ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€í•´ì£¼ì„¸ìš”:
                ì§ˆë¬¸: {question}
                
                ì¸í„°ë„· ê²€ìƒ‰ ê²°ê³¼: {internet_result}
                
                ìœ„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ì‹¤ ê¸°ë°˜ìœ¼ë¡œ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
                ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ì‚¬ê³¼ë‚˜ 'ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤'ì™€ ê°™ì€ í‘œí˜„ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
                í•„ìš”í•œ ê²½ìš° í•µì‹¬ ì¶œì²˜ ë§í¬ë¥¼ í•¨ê»˜ ì œì‹œí•˜ì„¸ìš”.
                """
                enhanced_result = self.qa_chain({"query": enhanced_prompt})
                response_time = time.time() - start_time
                print(f"ğŸŒ ì¸í„°ë„· ê²€ìƒ‰ ê¸°ë°˜ ë‹µë³€ ì™„ë£Œ (ì‘ë‹µì‹œê°„: {response_time:.2f}ì´ˆ)")
                return enhanced_result['result']
            
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (KBì— ìˆëŠ” ê²½ìš°)
            prompt = f"""
            ë‹¤ìŒì€ ìŠ¤í…Œì´ë¸”ì½”ì¸ ìš©ì–´ ë°±ê³¼ì‚¬ì „ì— ëŒ€í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤.
            ì§ˆë¬¸: {question}
            
            ì œê³µëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
            ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , í•„ìš”ì‹œ ì˜ˆì‹œë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.
            ì´ ì§ˆë¬¸ì€ ë°±ê³¼ì‚¬ì „ì— í¬í•¨ëœ ë‚´ìš©ì´ë¯€ë¡œ ìƒì„¸í•˜ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.
            """
            
            # QA ì²´ì¸ ì‹¤í–‰
            result = self.qa_chain({"query": prompt})
            answer = result["result"]
            
            # ì§€ì‹ë² ì´ìŠ¤ì—ì„œ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì–»ì—ˆëŠ”ì§€ í™•ì¸
            if self._check_knowledge_coverage(question, answer):
                response_time = time.time() - start_time
                print(f"ğŸ’¾ DB ë‹µë³€ ì™„ë£Œ (ì‘ë‹µì‹œê°„: {response_time:.2f}ì´ˆ)")
                return answer
            else:
                # ì¸í„°ë„· ê²€ìƒ‰ìœ¼ë¡œ ë³´ì™„
                internet_result = self._search_internet(question)
                enhanced_prompt = f"""
                ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ë‹µë³€í•´ì£¼ì„¸ìš”:
                ì§ˆë¬¸: {question}
                
                ì¸í„°ë„· ê²€ìƒ‰ ê²°ê³¼: {internet_result}
                
                ìœ„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ì‹¤ ê¸°ë°˜ìœ¼ë¡œ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
                ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ì‚¬ê³¼ë‚˜ 'ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤'ì™€ ê°™ì€ í‘œí˜„ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
                í•„ìš”í•œ ê²½ìš° í•µì‹¬ ì¶œì²˜ ë§í¬ë¥¼ í•¨ê»˜ ì œì‹œí•˜ì„¸ìš”.
                """
                enhanced_result = self.qa_chain({"query": enhanced_prompt})
                response_time = time.time() - start_time
                print(f"ğŸŒ ì¸í„°ë„· ê²€ìƒ‰ ë³´ì™„ ë‹µë³€ ì™„ë£Œ (ì‘ë‹µì‹œê°„: {response_time:.2f}ì´ˆ)")
                return enhanced_result["result"]
            
        except Exception as e:
            response_time = time.time() - start_time
            print(f"âŒ ë‹µë³€ ìƒì„± ì˜¤ë¥˜ (ì‘ë‹µì‹œê°„: {response_time:.2f}ì´ˆ)")
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def get_similar_terms(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        """ìœ ì‚¬í•œ ìš©ì–´ ê²€ìƒ‰"""
        try:
            if not self.vector_store:
                return []
            
            # ìœ ì‚¬ë„ ê²€ìƒ‰
            docs = self.vector_store.similarity_search(query, k=top_k)
            
            similar_terms = []
            for doc in docs:
                # ë©”íƒ€ë°ì´í„°ì—ì„œ ìš©ì–´ ì •ë³´ ì¶”ì¶œ
                metadata = doc.metadata
                content = doc.page_content
                
                if metadata.get('term'):
                    # ìš©ì–´ê°€ ìˆëŠ” ê²½ìš°
                    similar_terms.append({
                        "term": metadata['term'],
                        "section": metadata.get('section', ''),
                        "content": content[:300] + "..." if len(content) > 300 else content,
                        "similarity_score": 0.9,
                        "term_type": metadata.get('term_type', 'general')
                    })
                elif "**" in content:
                    # ë§ˆí¬ë‹¤ìš´ ë³¼ë“œ í…ìŠ¤íŠ¸ì—ì„œ ìš©ì–´ ì¶”ì¶œ
                    start = content.find("**") + 2
                    end = content.find("**", start)
                    if end > start:
                        term = content[start:end].strip()
                        similar_terms.append({
                            "term": term,
                            "section": metadata.get('section', ''),
                            "content": content[:300] + "..." if len(content) > 300 else content,
                            "similarity_score": 0.8,
                            "term_type": metadata.get('term_type', 'general')
                        })
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            unique_terms = []
            seen_terms = set()
            for term_data in similar_terms:
                if term_data['term'] not in seen_terms:
                    unique_terms.append(term_data)
                    seen_terms.add(term_data['term'])
            
            return unique_terms[:top_k]
            
        except Exception as e:
            print(f"ìœ ì‚¬ ìš©ì–´ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def search_terms_by_category(self, category: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """ì¹´í…Œê³ ë¦¬ë³„ ìš©ì–´ ê²€ìƒ‰"""
        try:
            if not self.vector_store:
                return []
            
            # ì¹´í…Œê³ ë¦¬ ê´€ë ¨ ê²€ìƒ‰
            query = f"ì„¹ì…˜: {category}"
            docs = self.vector_store.similarity_search(query, k=top_k)
            
            category_terms = []
            for doc in docs:
                metadata = doc.metadata
                if metadata.get('section') == category and metadata.get('term'):
                    category_terms.append({
                        "term": metadata['term'],
                        "section": metadata['section'],
                        "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                        "term_type": metadata.get('term_type', 'general')
                    })
            
            return category_terms
            
        except Exception as e:
            print(f"ì¹´í…Œê³ ë¦¬ë³„ ìš©ì–´ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
    
    def get_term_details(self, term: str) -> Dict[str, Any]:
        """íŠ¹ì • ìš©ì–´ì˜ ìƒì„¸ ì •ë³´ ì¡°íšŒ"""
        try:
            if not self.vector_store:
                return {}
            
            # ì •í™•í•œ ìš©ì–´ ê²€ìƒ‰
            query = f"ìš©ì–´: {term}"
            docs = self.vector_store.similarity_search(query, k=3)
            
            for doc in docs:
                metadata = doc.metadata
                if metadata.get('term') == term:
                    return {
                        "term": term,
                        "section": metadata.get('section', ''),
                        "definition": doc.page_content,
                        "term_type": metadata.get('term_type', 'general'),
                        "source": metadata.get('source', '')
                    }
            
            return {}
            
        except Exception as e:
            print(f"ìš©ì–´ ìƒì„¸ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
            return {}
    
    def get_all_categories(self) -> List[str]:
        """ëª¨ë“  ì¹´í…Œê³ ë¦¬(ì„¹ì…˜) ëª©ë¡ ì¡°íšŒ"""
        try:
            if not self.vector_store:
                return []
            
            # ëª¨ë“  ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ì—ì„œ ì„¹ì…˜ ì •ë³´ ìˆ˜ì§‘
            categories = set()
            
            # ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ëª¨ë“  ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ì— ì ‘ê·¼
            # FAISSëŠ” ì§ì ‘ì ì¸ ë©”íƒ€ë°ì´í„° ì ‘ê·¼ì´ ì œí•œì ì´ë¯€ë¡œ
            # ëŒ€ì‹  ìƒ˜í”Œ ê²€ìƒ‰ì„ í†µí•´ ì¹´í…Œê³ ë¦¬ ì •ë³´ ìˆ˜ì§‘
            sample_queries = ["ìŠ¤í…Œì´ë¸”ì½”ì¸", "ë¸”ë¡ì²´ì¸", "ê·œì œ", "ê¸°ìˆ ", "ì‹œì¥"]
            
            for query in sample_queries:
                docs = self.vector_store.similarity_search(query, k=5)
                for doc in docs:
                    if doc.metadata.get('section'):
                        categories.add(doc.metadata['section'])
            
            return sorted(list(categories))
            
        except Exception as e:
            print(f"ì¹´í…Œê³ ë¦¬ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
            return []

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_dictionary_instance = None

def get_dictionary_answer(question: str) -> str:
    """ìŠ¤í…Œì´ë¸”ì½”ì¸ ìš©ì–´ ë°±ê³¼ì‚¬ì „ì—ì„œ ë‹µë³€ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    global _dictionary_instance
    
    if _dictionary_instance is None:
        _dictionary_instance = StablecoinDictionary()
    
    return _dictionary_instance.get_answer(question)

def get_fast_dictionary_answer(question: str) -> str:
    """ìŠ¤í…Œì´ë¸”ì½”ì¸ ìš©ì–´ ë°±ê³¼ì‚¬ì „ì—ì„œ ë¹ ë¥¸ ë‹µë³€ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ (DBì— ìˆëŠ” ë‚´ìš©ì¸ ê²½ìš°)"""
    global _dictionary_instance
    
    if _dictionary_instance is None:
        _dictionary_instance = StablecoinDictionary()
    
    return _dictionary_instance.get_fast_answer(question)

def get_similar_terms(query: str, top_k: int = 5) -> List[Dict[str, Any]]:
    """ìœ ì‚¬í•œ ìš©ì–´ë¥¼ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜"""
    global _dictionary_instance
    
    if _dictionary_instance is None:
        _dictionary_instance = StablecoinDictionary()
    
    return _dictionary_instance.get_similar_terms(query, top_k)

def search_terms_by_category(category: str, top_k: int = 10) -> List[Dict[str, Any]]:
    """ì¹´í…Œê³ ë¦¬ë³„ ìš©ì–´ë¥¼ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜"""
    global _dictionary_instance
    
    if _dictionary_instance is None:
        _dictionary_instance = StablecoinDictionary()
    
    return _dictionary_instance.search_terms_by_category(category, top_k)

def get_term_details(term: str) -> Dict[str, Any]:
    """íŠ¹ì • ìš©ì–´ì˜ ìƒì„¸ ì •ë³´ë¥¼ ì¡°íšŒí•˜ëŠ” í•¨ìˆ˜"""
    global _dictionary_instance
    
    if _dictionary_instance is None:
        _dictionary_instance = StablecoinDictionary()
    
    return _dictionary_instance.get_term_details(term)

def get_all_categories() -> List[str]:
    """ëª¨ë“  ì¹´í…Œê³ ë¦¬ ëª©ë¡ì„ ì¡°íšŒí•˜ëŠ” í•¨ìˆ˜"""
    global _dictionary_instance
    
    if _dictionary_instance is None:
        _dictionary_instance = StablecoinDictionary()
    
    return _dictionary_instance.get_all_categories()

def is_question_in_kb(question: str) -> bool:
    """ì§ˆë¬¸ì´ KB(stablecoin_book_2025_full.md) ë²”ìœ„ì¸ì§€ ê³µê°œ í•¨ìˆ˜ë¡œ ì œê³µ"""
    global _dictionary_instance
    if _dictionary_instance is None:
        _dictionary_instance = StablecoinDictionary()
    return _dictionary_instance._is_in_knowledge_base(question)