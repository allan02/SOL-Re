import streamlit as st
import requests
import os
import pandas as pd
from email.utils import parsedate_to_datetime
from datetime import datetime, timezone, timedelta
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
try:
    from langgraph.graph import StateGraph, END
except ImportError:
    try:
        from langgraph.graph import Graph as StateGraph
        from langgraph.graph import END
    except ImportError:
        from langgraph import StateGraph, END
from typing import TypedDict, List, Dict, Any

# .env íŒŒì¼ ë¡œë“œ
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

excel_path = os.path.join(os.path.dirname(__file__), 'stablecoin_financial_institutions_with_roles_kor_renamed.xlsx')

@st.cache_data
def load_institutions_dataframe(path):
    return pd.read_excel(path, engine="openpyxl")

# State definition for LangGraph
class AnalysisState(TypedDict):
    company_name: str
    news_items: List[Dict[str, Any]]
    filtered_items: List[Dict[str, Any]]
    grouped_items: Dict[str, List[Dict[str, Any]]]
    group_analyses: Dict[str, str]
    issue_scores: Dict[str, int]
    current_step: str
    progress: int

def show_business_case_analysis():
    try:
        df = load_institutions_dataframe(excel_path)
        
        continent_col = "ëŒ€ë¥™"
        country_col = "êµ­ê°€"
        company_col = "íšŒì‚¬"

        df = df[[continent_col, country_col, company_col]].dropna(how="all")
        df[continent_col] = df[continent_col].astype(str).str.strip()
        df[country_col] = df[country_col].astype(str).str.strip()
        df[company_col] = df[company_col].astype(str).str.strip()

        all_label = "ì„ íƒ ì•ˆ í•¨"

        continents = [all_label] + sorted(df[continent_col].dropna().unique().tolist())
        col1, col2, col3 = st.columns(3)
        with col1:
            selected_continent = st.selectbox("ëŒ€ë¥™", options=continents, index=0)

        filtered_by_continent = df if selected_continent == all_label else df[df[continent_col] == selected_continent]

        countries = [all_label] + sorted(filtered_by_continent[country_col].dropna().unique().tolist())
        with col2:
            selected_country = st.selectbox("êµ­ê°€", options=countries, index=0)

        filtered_by_country = (
            filtered_by_continent
            if selected_country == all_label
            else filtered_by_continent[filtered_by_continent[country_col] == selected_country]
        )

        companies = [all_label] + sorted(filtered_by_country[company_col].dropna().unique().tolist())
        with col3:
            selected_company = st.selectbox("ê¸ˆìœµì‚¬", options=companies, index=0)
        
        # ê²€ìƒ‰ ë²„íŠ¼ì„ í† ê¸€ ëª©ë¡ ë°‘ì— ìµœëŒ€ í­ìœ¼ë¡œ ë°°ì¹˜
        role_search_clicked = st.button("ê²€ìƒ‰", key="role_search", use_container_width=True)
            
    except Exception as e:
        st.error(f"ì—‘ì…€ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

    # ì—­í•  ë¦¬ìŠ¤íŠ¸ (ìŠ¤í…Œì´ë¸” ì½”ì¸ ì‹œì¥ ë‚´ ì£¼ìš” ì—­í• )
    roles = [
        "ë°œí–‰",
        "ì¤€ë¹„ê¸ˆ",
        "ì¤€ë¹„ê¸ˆ ê´€ë¦¬",
        "ìˆ˜íƒ",
        "ê²°ì œ",
        "ê²°ì œ ì²˜ë¦¬",
        "ì§€ê°‘",
        "ê±°ë˜ì†Œ",
        "ì¸í”„ë¼",
        "ë¸”ë¡ì²´ì¸",
        "ì˜¨ë¨í”„",
        "ì˜¤í”„ë¨í”„",
        "ìœ ë™ì„±",
        "ìœ ë™ì„± ê³µê¸‰",
        "ë¦¬ìŠ¤í¬",
        "ë¦¬ìŠ¤í¬ ê´€ë¦¬",
    ]

    def is_within_last_month(pub_date_str):
        try:
            dt = parsedate_to_datetime(pub_date_str)
            cutoff = datetime.now(tz=timezone.utc) - timedelta(days=30)
            return dt >= cutoff
        except Exception:
            return False

    def fetch_news_items(query_text):
        url = "https://openapi.naver.com/v1/search/news.json"
        headers = {
            "X-Naver-Client-Id": "6nEOF04VFKYHVsqhgQ2I",
            "X-Naver-Client-Secret": "8_mjbLcEOy",
            "Content-Type": "plain/text"
        }
        params = {"query": query_text.strip(), "display": 5, "start": 1, "sort": "date"}
        try:
            response = requests.get(url, headers=headers, params=params)
            if response.status_code == 200:
                return response.json().get("items", [])
            return []
        except Exception:
            return []

    # LangGraph ë…¸ë“œ í•¨ìˆ˜ë“¤
    def fetch_news_node(state: AnalysisState) -> AnalysisState:
        """ë‰´ìŠ¤ ê²€ìƒ‰ ë…¸ë“œ"""
        company_name = state["company_name"]
        merged_items = []
        seen_links = set()
        
        # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
        state["current_step"] = f"ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘... ({company_name})"
        state["progress"] = 5
        
        for i, role in enumerate(roles):
            # ê° ì—­í• ë³„ ê²€ìƒ‰ ì§„í–‰ ìƒí™© í‘œì‹œ
            progress_per_role = 5 + (i / len(roles)) * 20
            state["progress"] = int(progress_per_role)
            state["current_step"] = f"ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘... {role} ({i+1}/{len(roles)})"
            
            query_text = f"{company_name} ìŠ¤í…Œì´ë¸” ì½”ì¸ {role}"
            items = fetch_news_items(query_text)
            for item in items:
                link = item.get("link")
                pub_date = item.get("pubDate")
                if link and link not in seen_links and pub_date and is_within_last_month(pub_date):
                    merged_items.append(item)
                    seen_links.add(link)
        
        # ìµœì‹ ìˆœ ì •ë ¬
        try:
            merged_items.sort(
                key=lambda x: parsedate_to_datetime(x.get("pubDate")) if x.get("pubDate") else datetime.min.replace(tzinfo=timezone.utc),
                reverse=True,
            )
        except Exception:
            pass
        
        state["news_items"] = merged_items
        state["current_step"] = f"ë‰´ìŠ¤ ê²€ìƒ‰ ì™„ë£Œ - {len(merged_items)}ê±´ ë°œê²¬"
        state["progress"] = 25
        return state

    def filter_relevance_node(state: AnalysisState) -> AnalysisState:
        """LLM ê¸°ë°˜ ê´€ë ¨ì„± í•„í„°ë§ ë…¸ë“œ"""
        company_name = state["company_name"]
        items = state["news_items"]
        
        # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
        state["current_step"] = f"ê´€ë ¨ì„± í•„í„°ë§ ì‹œì‘ - {len(items)}ê±´ ë¶„ì„ ì˜ˆì •"
        state["progress"] = 25
        
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            st.warning("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•Šì•„ LLM í•„í„°ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            state["filtered_items"] = items
            state["current_step"] = "LLM í•„í„°ë§ ê±´ë„ˆëœ€ - API í‚¤ ì—†ìŒ"
            state["progress"] = 50
            return state
        
        try:
            prompt = ChatPromptTemplate.from_messages([
                ("system", "ë„ˆëŠ” ë‰´ìŠ¤ ê¸°ì‚¬ì™€ íšŒì‚¬ëª… ê°„ì˜ ì§ì ‘ ê´€ë ¨ì„±ì„ íŒë‹¨í•˜ëŠ” í•„í„°ë‹¤. \n"
                            "ì§ì ‘ì ìœ¼ë¡œ í•´ë‹¹ íšŒì‚¬ì˜ ìŠ¤í…Œì´ë¸” ì½”ì¸ í™œë™(ë°œí–‰, ì¤€ë¹„ê¸ˆ, ê²°ì œ, ì§€ê°‘, ê±°ë˜ì†Œ, ì¸í”„ë¼ ë“±)ê³¼ ê´€ë ¨ëœ ê¸°ì‚¬ë§Œ 'YES'ë¡œ ì‘ë‹µí•˜ê³ , ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ 'NO'ë¡œ ì‘ë‹µí•œë‹¤. \n"
                            "ê´‘ë²”ìœ„í•œ ì—…ê³„ ë™í–¥, íƒ€ì‚¬ ì¤‘ì‹¬ ê¸°ì‚¬, ë‹¨ìˆœ ì–¸ê¸‰ì€ 'NO'ë¡œ ë¶„ë¥˜í•œë‹¤. \n"
                            "ë°˜ë“œì‹œ ëŒ€ë¬¸ì YES ë˜ëŠ” NO ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥í•œë‹¤."),
                ("human", "íšŒì‚¬: {company}\nì œëª©: {title}\në‚´ìš©: {description}")
            ])
            llm = ChatOpenAI(model="gpt-4o", temperature=0)
            chain = prompt | llm | StrOutputParser()
            
            filtered = []
            for i, item in enumerate(items):
                # ê° ê¸°ì‚¬ë³„ ì§„í–‰ ìƒí™© í‘œì‹œ
                progress_per_item = 25 + (i / len(items)) * 25
                state["progress"] = int(progress_per_item)
                state["current_step"] = f"ê´€ë ¨ì„± í•„í„°ë§ ì¤‘... ({i+1}/{len(items)})"
                
                title = item.get("title", "")
                description = item.get("description", "")
                try:
                    verdict = chain.invoke({
                        "company": company_name,
                        "title": title,
                        "description": description,
                    }).strip().upper()
                except Exception:
                    verdict = "NO"
                if verdict == "YES":
                    filtered.append(item)
            
            state["filtered_items"] = filtered
            state["current_step"] = f"LLM í•„í„°ë§ ì™„ë£Œ - {len(filtered)}ê±´ í•„í„°ë§ë¨"
            state["progress"] = 50
        except Exception:
            state["filtered_items"] = items
            state["current_step"] = "LLM í•„í„°ë§ ì‹¤íŒ¨ - ì˜¤ë¥˜ ë°œìƒ"
            state["progress"] = 50
        
        return state

    def group_issues_node(state: AnalysisState) -> AnalysisState:
        """LLM ê¸°ë°˜ ì´ìŠˆ ê·¸ë£¨í•‘ ë…¸ë“œ"""
        company_name = state["company_name"]
        items = state["filtered_items"]
        
        # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
        state["current_step"] = f"ì´ìŠˆ ê·¸ë£¨í•‘ ì‹œì‘ - {len(items)}ê±´ ë¶„ë¥˜ ì˜ˆì •"
        state["progress"] = 50
        
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            state["grouped_items"] = {"ê¸°íƒ€": items}
            state["current_step"] = "ì´ìŠˆ ê·¸ë£¨í•‘ ê±´ë„ˆëœ€ - API í‚¤ ì—†ìŒ"
            state["progress"] = 75
            return state
        
        try:
            prompt = ChatPromptTemplate.from_messages([
                ("system",
                 "ë„ˆëŠ” ìŠ¤í…Œì´ë¸” ì½”ì¸ ê´€ë ¨ ê¸°ì‚¬ë“¤ì„ ì´ìŠˆ(í† í”½)ë¡œ ë¼ë²¨ë§í•˜ëŠ” ë¶„ë¥˜ê¸°ë‹¤.\n"
                 "ë‹¤ìŒ ì›ì¹™ì„ ë”°ë¥´ë¼:\n"
                 "- ë°˜ë“œì‹œ í•œ ì¤„ë¡œ ê°„ê²°í•œ í•œêµ­ì–´ ë¼ë²¨ë§Œ ì¶œë ¥í•œë‹¤ (ì˜ˆ: 'ì¤€ë¹„ê¸ˆ ê·œì œ', 'í•´ì™¸ ê²°ì œ íŒŒíŠ¸ë„ˆì‹­', 'ê±°ë˜ì†Œ ìƒì¥', 'ì§€ê°‘ ì„œë¹„ìŠ¤ ì¶œì‹œ').\n"
                 "- ê°™ì€ ì´ìŠˆëŠ” ë™ì¼í•œ ë¼ë²¨ì„ ì‚¬ìš©í•´ ì¼ê´€ì„± ìˆê²Œ ë¶„ë¥˜í•œë‹¤.\n"
                 "- íšŒì‚¬ì˜ ìŠ¤í…Œì´ë¸” ì½”ì¸ í™œë™ ë§¥ë½ì„ ë°˜ì˜í•œë‹¤.\n"
                 "- ë¼ë²¨ ì™¸ ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” ì¶œë ¥í•˜ì§€ ì•ŠëŠ”ë‹¤."),
                ("human", "íšŒì‚¬: {company}\nì œëª©: {title}\në‚´ìš©: {description}")
            ])
            llm = ChatOpenAI(model="gpt-4o", temperature=0)
            chain = prompt | llm | StrOutputParser()
            
            groups = {}
            for i, item in enumerate(items):
                # ê° ê¸°ì‚¬ë³„ ì§„í–‰ ìƒí™© í‘œì‹œ
                progress_per_item = 50 + (i / len(items)) * 25
                state["progress"] = int(progress_per_item)
                state["current_step"] = f"ì´ìŠˆ ê·¸ë£¨í•‘ ì¤‘... ({i+1}/{len(items)})"
                
                title = item.get("title", "")
                description = item.get("description", "")
                try:
                    label = chain.invoke({
                        "company": company_name,
                        "title": title,
                        "description": description,
                    }).strip()
                    if not label:
                        label = "ê¸°íƒ€"
                except Exception:
                    label = "ê¸°íƒ€"
                item["_group"] = label
                groups.setdefault(label, []).append(item)
            
            # í° ê·¸ë£¹ ìš°ì„ ìœ¼ë¡œ ì •ë ¬ëœ dict ìƒì„±
            sorted_labels = sorted(groups.keys(), key=lambda k: len(groups[k]), reverse=True)
            state["grouped_items"] = {label: groups[label] for label in sorted_labels}
            state["current_step"] = f"ì´ìŠˆ ê·¸ë£¨í•‘ ì™„ë£Œ - {len(groups)}ê°œ ê·¸ë£¹ ìƒì„±"
            state["progress"] = 75
        except Exception:
            state["grouped_items"] = {"ê¸°íƒ€": items}
            state["current_step"] = "ì´ìŠˆ ê·¸ë£¨í•‘ ì‹¤íŒ¨ - ì˜¤ë¥˜ ë°œìƒ"
            state["progress"] = 75
        
        return state

    def analyze_groups_node(state: AnalysisState) -> AnalysisState:
        """ê·¸ë£¹ë³„ í†µí•© ë¶„ì„ ë…¸ë“œ"""
        company_name = state["company_name"]
        grouped_items = state["grouped_items"]
        
        # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
        state["current_step"] = f"ê·¸ë£¹ ë¶„ì„ ì‹œì‘ - {len(grouped_items)}ê°œ ê·¸ë£¹ ë¶„ì„ ì˜ˆì •"
        state["progress"] = 75
        
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            group_analyses = {}
            for label, items_in_group in grouped_items.items():
                group_analyses[label] = f"{label} ê´€ë ¨ ê¸°ì‚¬ {len(items_in_group)}ê±´ ìš”ì•½ ì œê³µ (API Key ì—†ìŒìœ¼ë¡œ ê°„ë‹¨ í‘œì‹œ)."
            state["group_analyses"] = group_analyses
            state["current_step"] = "ê·¸ë£¹ ë¶„ì„ ê±´ë„ˆëœ€ - API í‚¤ ì—†ìŒ"
            state["progress"] = 90
            return state
        
        try:
            prompt = ChatPromptTemplate.from_messages([
                ("system",
                    "ë„ˆëŠ” ìŠ¤í…Œì´ë¸” ì½”ì¸ ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ë¶„ì„í•˜ì—¬ ì‹ í•œê¸ˆìœµê·¸ë£¹ ì…ì¥ì—ì„œì˜ ì¢…í•©ì ì¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ëŠ” ì „ë¬¸ê°€ë‹¤. \n"
                    "ë‹¤ìŒ ì„¸ ê°€ì§€ë¥¼ í•˜ë‚˜ì˜ ì‘ë‹µìœ¼ë¡œ ì‘ì„±í•´ë¼: \n\n"
                    "**ì´ìŠˆ ìš”ì•½** (200ì ì´ë‚´)\n"
                    "1. í•µì‹¬ ì´ìŠˆ: ë¬´ì—‡ì´ ì¼ì–´ë‚¬ëŠ”ì§€\n"
                    "2. ì£¼ìš” ì°¸ì—¬ì: ëˆ„ê°€ ê´€ë ¨ë˜ì–´ ìˆëŠ”ì§€\n"
                    "3. ì‹œì¥ ì˜í–¥: ì´ ì´ìŠˆê°€ ìŠ¤í…Œì´ë¸” ì½”ì¸ ì‹œì¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥\n"
                    "4. í–¥í›„ ì „ë§: ì•ìœ¼ë¡œ ì–´ë–»ê²Œ ë  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ”ì§€\n\n"
                    "**ì‹ í•œê¸ˆìœµê·¸ë£¹ ì…ì¥ì—ì„œì˜ ì˜í–¥ë„ íŒë‹¨**\n"
                    "ì´ ë‰´ìŠ¤ ì´ìŠˆê°€ ì‹ í•œê¸ˆìœµê·¸ë£¹ì—ê²Œ ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ íŒë‹¨:\n"
                    "- ë§¤ìš° ë¶€ì •ì : ì‹ í•œê¸ˆìœµê·¸ë£¹ì˜ ì‚¬ì—…ì— ì‹¬ê°í•œ ìœ„í˜‘ì´ë‚˜ ì†ì‹¤ì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆëŠ” ì´ìŠˆ\n"
                    "- ë¶€ì •ì : ì‹ í•œê¸ˆìœµê·¸ë£¹ì˜ ì‚¬ì—…ì— ë¶€ì •ì  ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆëŠ” ì´ìŠˆ\n"
                    "- ì¤‘ë¦½: ì‹ í•œê¸ˆìœµê·¸ë£¹ì˜ ì‚¬ì—…ì— ì§ì ‘ì ì¸ ì˜í–¥ì´ ë¯¸ë¯¸í•˜ê±°ë‚˜ ë¶ˆë¶„ëª…í•œ ì´ìŠˆ\n"
                    "- ê¸ì •ì : ì‹ í•œê¸ˆìœµê·¸ë£¹ì˜ ì‚¬ì—…ì— ê¸ì •ì  ê¸°íšŒë¥¼ ì œê³µí•  ìˆ˜ ìˆëŠ” ì´ìŠˆ\n"
                    "- ë§¤ìš° ê¸ì •ì : ì‹ í•œê¸ˆìœµê·¸ë£¹ì˜ ì‚¬ì—…ì— í° ì„±ì¥ ê¸°íšŒë‚˜ ì´ìµì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆëŠ” ì´ìŠˆ\n\n"
                    "**ì‹ í•œê¸ˆìœµê·¸ë£¹ì´ ì–»ì„ ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ ì¸ì‚¬ì´íŠ¸** (400ì ì´ë‚´)\n"
                    "1. ê²½ìŸì‚¬ ë¶„ì„: ê²½ìŸì‚¬ë“¤ì˜ êµ¬ì²´ì ì¸ ì›€ì§ì„, ì „ëµ, ì‹œì¥ í¬ì§€ì…”ë‹\n"
                    "2. ìœ„í—˜ ìš”ì†Œ: ì£¼ì˜í•´ì•¼ í•  êµ¬ì²´ì ì¸ ë¦¬ìŠ¤í¬, ê·œì œ ë³€í™”, ì‹œì¥ ë¶ˆí™•ì‹¤ì„±\n"
                    "3. ì‹œì¥ í¬ì§€ì…”ë‹: ì‹ í•œê¸ˆìœµê·¸ë£¹ì´ ì°¨ë³„í™”í•  ìˆ˜ ìˆëŠ” ì˜ì—­ê³¼ í•µì‹¬ ì—­ëŸ‰\n\n"
                    "**ì¶”ì²œ ì¡°ì‚¬ ì§ˆì˜** (ìµœëŒ€ 2ê°œ)\n"
                    "í•´ë‹¹ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì˜ ì–´ë– í•œ ë¬¸êµ¬ì„ ê³ ë ¤í–ˆì„ ë–„, ì‹ í•œê¸ˆìœµê·¸ë£¹ ì…ì¥ì—ì„œ ì¶”ê°€ì ìœ¼ë¡œ ì¡°ì‚¬ê°€ í•„ìš”í•œ ì£¼ì œë¡œ ì§ˆë¬¸ì„ ë§Œë“¤ì–´ë¼."
                    "ì§ˆì˜ëŠ” ì™„ì„±ëœ í•œ ê°œì˜ ë¬¸ì¥ í˜•ì‹ìœ¼ë¡œ ë§Œë“¤ì–´ë¼."
                    "ì§ˆì˜ëŠ” êµ¬ì²´ì ì¼ ìˆ˜ë¡ ì¢‹ë‹¤."
                    "ê° ì„¹ì…˜ì€ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ê³ , ì‹ í•œê¸ˆìœµê·¸ë£¹ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì ì—ì„œ ì‹¤ìš©ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ë¼."),
                ("human",
                 "íšŒì‚¬: {company}\nì´ìŠˆ ë¼ë²¨: {label}\nê¸°ì‚¬ ì œëª©ë“¤: {titles}\nê¸°ì‚¬ ìš”ì•½ë“¤: {descriptions}\nìœ„ ë‚´ìš©ì„ ì¢…í•© ë¶„ì„:")
            ])
            llm = ChatOpenAI(model="gpt-4o", temperature=0.3)
            chain = prompt | llm | StrOutputParser()
            
            group_analyses = {}
            for i, (label, items_in_group) in enumerate(grouped_items.items()):
                # ê° ê·¸ë£¹ë³„ ì§„í–‰ ìƒí™© í‘œì‹œ
                progress_per_group = 75 + (i / len(grouped_items)) * 15
                state["progress"] = int(progress_per_group)
                state["current_step"] = f"ê·¸ë£¹ ë¶„ì„ ì¤‘... {label} ({i+1}/{len(grouped_items)})"
                
                titles = " | ".join([(i.get("title") or "") for i in items_in_group])
                descriptions = " | ".join([(i.get("description") or "") for i in items_in_group])
                try:
                    analysis = chain.invoke({
                        "company": company_name,
                        "label": label,
                        "titles": titles,
                        "descriptions": descriptions,
                    }).strip()
                    group_analyses[label] = analysis
                except Exception:
                    group_analyses[label] = f"{label} ê´€ë ¨ ê¸°ì‚¬ {len(items_in_group)}ê±´."
            
            state["group_analyses"] = group_analyses
            state["current_step"] = f"ê·¸ë£¹ ë¶„ì„ ì™„ë£Œ - {len(group_analyses)}ê°œ ê·¸ë£¹ ë¶„ì„ ì™„ë£Œ"
            state["progress"] = 90
        except Exception:
            group_analyses = {}
            for label, items_in_group in grouped_items.items():
                group_analyses[label] = f"{label} ê´€ë ¨ ê¸°ì‚¬ {len(items_in_group)}ê±´."
            state["group_analyses"] = group_analyses
            state["current_step"] = "ê·¸ë£¹ ë¶„ì„ ì‹¤íŒ¨ - ì˜¤ë¥˜ ë°œìƒ"
            state["progress"] = 90
        
        return state

    def score_importance_node(state: AnalysisState) -> AnalysisState:
        """ì´ìŠˆ ì¤‘ìš”ë„ ì ìˆ˜í™” ë…¸ë“œ"""
        grouped_items = state["grouped_items"]
        
        # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
        state["current_step"] = f"ì¤‘ìš”ë„ ì ìˆ˜í™” ì‹œì‘ - {len(grouped_items)}ê°œ ê·¸ë£¹ í‰ê°€ ì˜ˆì •"
        state["progress"] = 90
        
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            issue_scores = {}
            for label, items_in_group in grouped_items.items():
                article_count = len(items_in_group)
                total_length = sum(len((i.get("title") or "")) + len((i.get("description") or "")) for i in items_in_group)
                heuristic_score = min(100, (article_count * 10) + (total_length // 100))
                issue_scores[label] = heuristic_score
            state["issue_scores"] = issue_scores
            state["current_step"] = "ì¤‘ìš”ë„ ì ìˆ˜í™” ì™„ë£Œ - API í‚¤ ì—†ìŒìœ¼ë¡œ íœ´ë¦¬ìŠ¤í‹± ì ìˆ˜ ì‚¬ìš©"
            state["progress"] = 100
            return state
        
        try:
            prompt = ChatPromptTemplate.from_messages([
                ("system",
                 "ë„ˆëŠ” ìŠ¤í…Œì´ë¸” ì½”ì¸ ì´ìŠˆì˜ ì¤‘ìš”ë„ë¥¼ 0~100ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ì‹¬ì‚¬ìœ„ì›ì´ë‹¤.\n"
                 "í‰ê°€ ê¸°ì¤€:\n"
                 "1) ìµœì‹  ê¸°ì‚¬ ë° ì •ë³´ì¼ìˆ˜ë¡ (ì •ë³´ì˜ ìµœì‹ ì„±)\n"
                 "2) ì´ìŠˆì— í¬í•¨ëœ ê¸°ì‚¬ ìˆ˜ê°€ ë§ì„ìˆ˜ë¡ ë” ì¤‘ìš” (ê¸°ì‚¬ ìˆ˜ ë¹„ë¡€)\n"
                 "3) ì‹ í•œê¸ˆìœµê·¸ë£¹ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì´ í´ìˆ˜ë¡ ë” ì¤‘ìš” (ì§ì ‘ ì˜í–¥ë„)\n"
                 "4) ì‹ í•œê¸ˆìœµê·¸ë£¹ì´ ë²¤ì¹˜ë§ˆí‚¹í•  ì—¬ì§€ê°€ ë§ì„ìˆ˜ë¡ ë” ì¤‘ìš” (í•™ìŠµ/ì ìš© ê°€ëŠ¥ì„±)\n"
                 "5) ê¸°ì‚¬ë“¤ì˜ ì´ ê¸¸ì´ê°€ ê¸¸ìˆ˜ë¡ ë” ì¤‘ìš” (ì •ë³´ëŸ‰)\n"
                 "ë°˜ë“œì‹œ 0~100 ì‚¬ì´ì˜ ì •ìˆ˜ í•˜ë‚˜ë§Œ ì¶œë ¥í•œë‹¤. ë‹¤ë¥¸ ë¬¸ìëŠ” ì¶œë ¥í•˜ì§€ ë§ˆë¼."),
                ("human",
                 "ì´ìŠˆ ë¼ë²¨: {label}\n"
                 "ê¸°ì‚¬ ìˆ˜: {article_count}\n"
                 "ê¸°ì‚¬ ì œëª©ë“¤: {titles}\n"
                 "ê¸°ì‚¬ ìš”ì•½ë“¤: {descriptions}\n"
                 "ì´ ê¸°ì‚¬ ê¸¸ì´(ë¬¸ììˆ˜): {total_length}\n"
                 "ìœ„ ê¸°ì¤€ì— ë”°ë¼ ì´ ì´ìŠˆì˜ ì¤‘ìš”ë„ë¥¼ 0~100 ì •ìˆ˜ë¡œë§Œ ì¶œë ¥:")
            ])
            llm = ChatOpenAI(model="gpt-4o", temperature=0)
            chain = prompt | llm | StrOutputParser()
            
            issue_scores = {}
            for i, (label, items_in_group) in enumerate(grouped_items.items()):
                # ê° ê·¸ë£¹ë³„ ì§„í–‰ ìƒí™© í‘œì‹œ
                progress_per_group = 90 + (i / len(grouped_items)) * 10
                state["progress"] = int(progress_per_group)
                state["current_step"] = f"ì¤‘ìš”ë„ ì ìˆ˜í™” ì¤‘... {label} ({i+1}/{len(grouped_items)})"
                
                titles = " | ".join([(i.get("title") or "") for i in items_in_group])
                descriptions = " | ".join([(i.get("description") or "") for i in items_in_group])
                article_count = len(items_in_group)
                total_length = sum(len((i.get("title") or "")) + len((i.get("description") or "")) for i in items_in_group)
                try:
                    out = chain.invoke({
                        "label": label,
                        "article_count": str(article_count),
                        "titles": titles,
                        "descriptions": descriptions,
                        "total_length": str(total_length),
                    }).strip()
                    # ì •ìˆ˜ íŒŒì‹±
                    score_val = int(''.join([c for c in out if c.isdigit()]) or 0)
                    score_val = max(0, min(score_val, 100))
                    issue_scores[label] = score_val
                except Exception:
                    heuristic_score = min(100, (article_count * 10) + (total_length // 100))
                    issue_scores[label] = heuristic_score
            
            state["issue_scores"] = issue_scores
            state["current_step"] = f"ì¤‘ìš”ë„ ì ìˆ˜í™” ì™„ë£Œ - {len(issue_scores)}ê°œ ê·¸ë£¹ í‰ê°€ ì™„ë£Œ"
            state["progress"] = 100
        except Exception:
            issue_scores = {}
            for label, items_in_group in grouped_items.items():
                article_count = len(items_in_group)
                total_length = sum(len((i.get("title") or "")) + len((i.get("description") or "")) for i in items_in_group)
                heuristic_score = min(100, (article_count * 10) + (total_length // 100))
                issue_scores[label] = heuristic_score
            state["issue_scores"] = issue_scores
            state["current_step"] = "ì¤‘ìš”ë„ ì ìˆ˜í™” ì‹¤íŒ¨ - ì˜¤ë¥˜ ë°œìƒìœ¼ë¡œ íœ´ë¦¬ìŠ¤í‹± ì ìˆ˜ ì‚¬ìš©"
            state["progress"] = 100
        
        return state

    # LangGraph êµ¬ì„±
    def create_analysis_graph():
        """ë¶„ì„ì„ ìœ„í•œ LangGraph ìƒì„±"""
        workflow = StateGraph(AnalysisState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("fetch_news", fetch_news_node)
        workflow.add_node("filter_relevance", filter_relevance_node)
        workflow.add_node("group_issues", group_issues_node)
        workflow.add_node("analyze_groups", analyze_groups_node)
        workflow.add_node("score_importance", score_importance_node)
        
        # ì—£ì§€ ì—°ê²°
        workflow.set_entry_point("fetch_news")
        workflow.add_edge("fetch_news", "filter_relevance")
        workflow.add_edge("filter_relevance", "group_issues")
        workflow.add_edge("group_issues", "analyze_groups")
        workflow.add_edge("analyze_groups", "score_importance")
        workflow.add_edge("score_importance", END)
        
        return workflow.compile()

    # ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™©ì„ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜
    def show_progress_bar(current_step, progress):
        """í˜„ì¬ ì§„í–‰ ìƒí™©ì„ í”„ë¡œê·¸ë ˆìŠ¤ ë°”ë¡œ í‘œì‹œ"""
        st.progress(progress / 100)
        st.info(f"ğŸ”„ í˜„ì¬ ì²˜ë¦¬ ì¤‘: {current_step}")
        
    def show_step_details(step_name, details=""):
        """ê° ë‹¨ê³„ë³„ ìƒì„¸ ì •ë³´ í‘œì‹œ"""
        with st.expander(f"ğŸ“‹ {step_name} ìƒì„¸ ì •ë³´", expanded=True):
            if details:
                st.write(details)
            else:
                st.write("ì²˜ë¦¬ ì¤‘...")

    # ì„ íƒëœ íšŒì‚¬ëª… ê¸°ì¤€ ì—­í• ë³„ ê²€ìƒ‰ ë° ë³‘í•©
    try:
        if role_search_clicked:
            if not selected_company or selected_company == "ì„ íƒ ì•ˆ í•¨":
                st.warning("ê¸ˆìœµì‚¬ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            else:
                # ì§„í–‰ ìƒí™©ì„ í‘œì‹œí•  ì»¨í…Œì´ë„ˆ ìƒì„±
                progress_container = st.container()
                
                with progress_container:
                    st.markdown("**ë¶„ì„ ì§„í–‰ ìƒí™©**")
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                
                # ì´ˆê¸° ìƒíƒœ ì„¤ì •
                initial_state = AnalysisState(
                    company_name=selected_company,
                    news_items=[],
                    filtered_items=[],
                    grouped_items={},
                    group_analyses={},
                    issue_scores={},
                    current_step="ì‹œì‘",
                    progress=0
                )
                
                # LangGraph ì‹¤í–‰ (ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© í‘œì‹œ)
                graph = create_analysis_graph()
                
                # ê° ë…¸ë“œ ì‹¤í–‰ í›„ ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                current_state = initial_state
                
                # 1ë‹¨ê³„: ë‰´ìŠ¤ ê²€ìƒ‰
                current_state = fetch_news_node(current_state)
                with progress_container:
                    progress_bar.progress(current_state["progress"] / 100)
                    status_text.info(f"{current_state['current_step']}")
                
                # 2ë‹¨ê³„: ê´€ë ¨ì„± í•„í„°ë§
                current_state = filter_relevance_node(current_state)
                with progress_container:
                    progress_bar.progress(current_state["progress"] / 100)
                    status_text.info(f"{current_state['current_step']}")
                
                # 3ë‹¨ê³„: ì´ìŠˆ ê·¸ë£¨í•‘
                current_state = group_issues_node(current_state)
                with progress_container:
                    progress_bar.progress(current_state["progress"] / 100)
                    status_text.info(f"{current_state['current_step']}")
                
                # 4ë‹¨ê³„: ê·¸ë£¹ ë¶„ì„
                current_state = analyze_groups_node(current_state)
                with progress_container:
                    progress_bar.progress(current_state["progress"] / 100)
                    status_text.info(f"{current_state['current_step']}")
                
                # 5ë‹¨ê³„: ì¤‘ìš”ë„ ì ìˆ˜í™”
                current_state = score_importance_node(current_state)
                with progress_container:
                    progress_bar.progress(current_state["progress"] / 100)
                    status_text.success(f"{current_state['current_step']}")
                
                # ìµœì¢… ìƒíƒœë¥¼ final_stateë¡œ ì„¤ì •
                final_state = current_state
                
                # ê²°ê³¼ ì¶”ì¶œ
                merged_items = final_state["filtered_items"]
                grouped = final_state["grouped_items"]
                group_analyses = final_state["group_analyses"]
                issue_scores = final_state["issue_scores"]

                st.success(f"ê²€ìƒ‰ ê²°ê³¼ {len(merged_items)}ê±´ (ìµœê·¼ 1ê°œì›” ë‚´)")
                
                st.markdown("---")
                
                # Top 3 ì´ìŠˆë§Œ ë…¸ì¶œ
                top_3_labels = list(grouped.keys())[:3]
                
                for label in top_3_labels:
                    items_in_group = grouped[label]
                    importance_score = issue_scores.get(label, 0)
                    
                    st.subheader(f"ì´ìŠˆ: {label}")
                    st.caption(f"ì¤‘ìš”ë„ ì ìˆ˜: {importance_score}/100")
                    
                    # ì™¼ìª½: ìš”ì•½/ì¸ì‚¬ì´íŠ¸/ì¶”ì²œ ì¡°ì‚¬, ì˜¤ë¥¸ìª½: ê¸°ì‚¬ ëª©ë¡
                    col_analysis, col_articles = st.columns([2, 1])
                    
                    with col_analysis:
                        # í†µí•© ë¶„ì„ ê²°ê³¼ í‘œì‹œ
                        analysis_text = group_analyses.get(label) or "ë¶„ì„ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                        st.markdown(analysis_text)
                    
                    with col_articles:
                        # ê´€ë ¨ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
                        st.write("**ê´€ë ¨ ê¸°ì‚¬:**")
                        for item in items_in_group:
                            st.markdown(f"**{item.get('title', '')}**", unsafe_allow_html=True)
                            st.markdown(item.get('description', ''), unsafe_allow_html=True)
                            link = item.get('link', '')
                            if link:
                                st.markdown(f"[ê¸°ì‚¬ ë³´ê¸°]({link})")
                            pub_date = item.get('pubDate')
                            if pub_date:
                                try:
                                    dt = parsedate_to_datetime(pub_date)
                                    st.caption(dt.strftime('%Y-%m-%d %H:%M'))
                                except Exception:
                                    st.caption(pub_date)
                            st.divider()
    except Exception as e:
        st.error(f"ì—­í• ë³„ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")